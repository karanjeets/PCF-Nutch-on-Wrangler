<?xml version="1.0"?>
<?xml-stylesheet type="text/xsl" href="configuration.xsl"?>
<!--
 Licensed to the Apache Software Foundation (ASF) under one or more
 contributor license agreements.  See the NOTICE file distributed with
 this work for additional information regarding copyright ownership.
 The ASF licenses this file to You under the Apache License, Version 2.0
 (the "License"); you may not use this file except in compliance with
 the License.  You may obtain a copy of the License at

     http://www.apache.org/licenses/LICENSE-2.0

 Unless required by applicable law or agreed to in writing, software
 distributed under the License is distributed on an "AS IS" BASIS,
 WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 See the License for the specific language governing permissions and
 limitations under the License.
-->
<!-- Do not modify this file directly.  Instead, copy entries that you -->
<!-- wish to modify from this file into nutch-site.xml and change them -->
<!-- there.  If nutch-site.xml does not already exist, create it.      -->

<configuration>

<!-- file properties -->

<property>
  <name>file.content.limit</name>
  <value>900000</value><!-- 10000 -->
  <description>The length limit for downloaded content using the file://
  protocol, in bytes. If this value is nonnegative (>=0), content longer
  than it will be truncated; otherwise, no truncation at all. Do not
  confuse this setting with the http.content.limit setting.
  </description>
</property>

<!-- HTTP properties -->

<property>
  <name>http.agent.name</name>
  <value>team10_dexter</value>
  <description>HTTP 'User-Agent' request header. MUST NOT be empty - 
  please set this to a single word uniquely related to your organization.

  NOTE: You should also check other related properties:

    http.robots.agents
    http.agent.description
    http.agent.url
    http.agent.email
    http.agent.version

  and set their values appropriately.

  </description>
</property>

<property>
  <name>http.robot.rules.whitelist</name>
  <value>www.impactguns.com,www.nationalguntrader.com,www.shooterswap.com,www.slickguns.com,www.shooting.org</value>
  <description>Comma separated list of hostnames or IP addresses to ignore 
  robot rules parsing for. Use with care and only if you are explicitly
  allowed by the site owner to ignore the site's robots.txt!
  </description>
</property>

<property>
  <name>http.agent.description</name>
  <value></value>
  <description>Further description of our bot- this text is used in
  the User-Agent header.  It appears in parenthesis after the agent name.
  </description>
</property>

<property>
  <name>http.agent.url</name>
  <value></value>
  <description>A URL to advertise in the User-Agent header.  This will 
   appear in parenthesis after the agent name. Custom dictates that this
   should be a URL of a page explaining the purpose and behavior of this
   crawler.
  </description>
</property>

<property>
  <name>http.agent.email</name>
  <value>shoot.dexter@gmail.com</value>
  <description>An email address to advertise in the HTTP 'From' request
   header and User-Agent header. A good practice is to mangle this
   address (e.g. 'info at example dot com') to avoid spamming.
  </description>
</property>

<property>
  <name>http.agent.version</name>
  <value>1.0</value>
  <description>A version string to advertise in the User-Agent 
   header.</description>
</property>

<property>
  <name>http.agent.rotate</name>
  <value>true</value>
  <description>
    If true, instead of http.agent.name, alternating agent names are
    chosen from a list provided via http.agent.rotate.file.
  </description>
</property>

<property>
  <name>http.agent.rotate.file</name>
  <value>agents.txt</value>
  <description>
    File containing alternative user agent names to be used instead of
    http.agent.name on a rotating basis if http.agent.rotate is true.
    Each line of the file should contain exactly one agent
    specification including name, version, description, URL, etc.
  </description>
</property>

<property>
  <name>http.agent.host</name>
  <value>USC</value>
  <description>Name or IP address of the host on which the Nutch crawler
  would be running. Currently this is used by 'protocol-httpclient'
  plugin.
  </description>
</property>

<property>
  <name>http.timeout</name>
  <value>10000</value>
  <description>The default network timeout, in milliseconds.</description>
</property>

<property>
  <name>http.max.delays</name>
  <value>10</value> <!-- Changed from 25 -->
  <description>The number of times a thread will delay when trying to
  fetch a page.  Each time it finds that a host is busy, it will wait
  fetcher.server.delay.  After http.max.delays attepts, it will give
  up on the page for now.</description>
</property>

<property>
  <name>http.content.limit</name>
  <value>900000</value><!-- 10000 -->
  <description>The length limit for downloaded content using the http://
  protocol, in bytes. If this value is nonnegative (>=0), content longer
  than it will be truncated; otherwise, no truncation at all. Do not
  confuse this setting with the file.content.limit setting.
  </description>
</property>

<property>
  <name>http.auth.file</name>
  <value>httpclient-auth.xml</value>
  <description>Authentication configuration file for
  'protocol-httpclient' plugin.
  </description>
</property>

<property>
  <name>http.verbose</name>
  <value>true</value>
  <description>If true, HTTP will log more verbosely.</description>
</property>

<property>
  <name>http.redirect.max</name>
  <value>0</value><!-- 0 -->
  <description>The maximum number of redirects the fetcher will follow when
  trying to fetch a page. If set to negative or 0, fetcher won't immediately
  follow redirected URLs, instead it will record them for later fetching.
  </description>
</property>

<property>
  <name>http.useHttp11</name>
  <value>true</value>
  <description>NOTE: at the moment this works only for protocol-httpclient.
  If true, use HTTP 1.1, if false use HTTP 1.0 .
  </description>
</property>

<!-- FTP properties -->

<!-- web db properties -->


<property>
  <name>db.preserve.backup</name>
  <value>false</value>
  <description>If true, updatedb will keep a backup of the previous CrawlDB
  version in the old directory. In case of disaster, one can rename old to 
  current and restore the CrawlDB to its previous state.
  </description>
</property>

<property>
    <name>db.url.normalizers</name>
    <value>true</value>
    <description>Normalize urls when updating crawldb</description>
</property>

<property>
    <name>db.url.filters</name>
    <value>true</value>
    <description>Filter urls when updating crawldb</description>
</property>

<property>
  <name>db.ignore.internal.links</name>
  <value>false</value>
  <description>If true, when adding new links to a page, links from
  the same host are ignored.  This is an effective way to limit the
  size of the link database, keeping only the highest quality
  links.
  </description>
</property>

 <property>
  <name>db.injector.overwrite</name>
  <value>false</value> <!-- Changed from true -->
  <description>Whether existing records in the CrawlDB will be overwritten
  by injected records.
  </description>
</property>

<property>
  <name>db.max.outlinks.per.page</name>
  <value>-1</value>
  <description>The maximum number of outlinks that we'll process for a page.
  If this value is nonnegative (>=0), at most db.max.outlinks.per.page outlinks
  will be processed for a page; otherwise, all outlinks will be processed.
  </description>
</property>

<property>
  <name>db.max.anchor.length</name>
  <value>500</value>
  <description>The maximum number of characters permitted in an anchor.
  </description>
</property>

<property>
  <name>db.fetch.interval.default</name>
  <value>2592000</value> <!-- Changed from 2592000 -->
  <description>The default number of seconds between re-fetches of a page (30 days).
  </description>
</property>

<!-- generate properties -->

<property>
  <name>generate.update.crawldb</name>
  <value>true</value>
  <description>For highly-concurrent environments, where several
  generate/fetch/update cycles may overlap, setting this to true ensures
  that generate will create different fetchlists even without intervening
  updatedb-s, at the cost of running an additional job to update CrawlDB.
  If false, running generate twice without intervening
  updatedb will generate identical fetchlists.</description>
</property>

<!-- urlpartitioner properties -->

<!-- fetcher properties -->

<property>
  <name>fetcher.server.min.delay</name>
  <value>6.0</value>
  <description>The minimum number of seconds the fetcher will delay between 
  successive requests to the same server. This value is applicable ONLY
  if fetcher.threads.per.queue is greater than 1 (i.e. the host blocking
  is turned off).</description>
</property>

<property>
 <name>fetcher.max.crawl.delay</name>
 <value>10</value><!-- 0 -->
 <description>
 If the Crawl-Delay in robots.txt is set to greater than this value (in
 seconds) then the fetcher will skip this page, generating an error report.
 If set to -1 the fetcher will never skip such pages and will wait the
 amount of time retrieved from robots.txt Crawl-Delay, however long that
 might be.
 </description>
</property> 

<property>
  <name>fetcher.threads.fetch</name>
  <value>1</value><!-- 10 -->
  <description>The number of FetcherThreads the fetcher should use.
  This is also determines the maximum number of requests that are
  made at once (each FetcherThread handles one connection). The total
  number of threads running in distributed mode will be the number of
  fetcher threads * number of nodes as fetcher has one map task per node.
  </description>
</property>

<property>
  <name>fetcher.threads.per.queue</name>
  <value>1</value><!-- 1 -->
  <description>This number is the maximum number of threads that
    should be allowed to access a queue at one time. Setting it to 
    a value > 1 will cause the Crawl-Delay value from robots.txt to
    be ignored and the value of fetcher.server.min.delay to be used
    as a delay between successive requests to the same server instead 
    of fetcher.server.delay.
   </description>
</property>

<property>
  <name>fetcher.verbose</name>
  <value>true</value>
  <description>If true, fetcher will log more verbosely.</description>
</property>

<property>
  <name>fetcher.parse</name>
  <value>false</value>
  <description>If true, fetcher will parse content. Default is false, which means
  that a separate parsing step is required after fetching is finished.</description>
</property>

<property>
  <name>fetcher.store.content</name>
  <value>true</value>
  <description>If true, fetcher will store content.</description>
</property>

<property>
  <name>fetcher.timelimit.mins</name>
  <value>-1</value>
  <description>This is the number of minutes allocated to the fetching.
  Once this value is reached, any remaining entry from the input URL list is skipped 
  and all active queues are emptied. The default value of -1 deactivates the time limit.
  </description>
</property>

<property>
  <name>fetcher.max.exceptions.per.queue</name>
  <value>3</value><!-- -1 -->
  <description>The maximum number of protocol-level exceptions (e.g. timeouts) per
  host (or IP) queue. Once this value is reached, any remaining entries from this
  queue are purged, effectively stopping the fetching from this host/IP. The default
  value of -1 deactivates this limit.
  </description>
</property>

<property>
  <name>fetcher.throughput.threshold.pages</name>
  <value>5</value><!-- -1 -->
  <description>The threshold of minimum pages per second. If the fetcher downloads less
  pages per second than the configured threshold, the fetcher stops, preventing slow queue's
  from stalling the throughput. This threshold must be an integer. This can be useful when
  fetcher.timelimit.mins is hard to determine. The default value of -1 disables this check.
  </description>
</property>

<property>
  <name>fetcher.follow.outlinks.ignore.external</name>
  <value>true</value> <!-- Changed from false --> 
  <description>Whether to ignore or follow external links. Set db.ignore.external.links to false and this to true to store outlinks
  in the output but not follow them. If db.ignore.external.links is true this directive is ignored.
  </description>
</property>

<!-- moreindexingfilter plugin properties -->

<property>
  <name>moreIndexingFilter.mapMimeTypes</name>
  <value>true</value>
  <description>Determines whether MIME-type mapping is enabled. It takes a
  plain text file with mapped MIME-types. With it the user can map both
  application/xhtml+xml and text/html to the same target MIME-type so it
  can be treated equally in an index. See conf/contenttype-mapping.txt.
  </description>
</property>

<!-- AnchorIndexing filter plugin properties -->

<property>
  <name>anchorIndexingFilter.deduplicate</name>
  <value>true</value>
  <description>With this enabled the indexer will case-insensitive deduplicate anchors
  before indexing. This prevents possible hundreds or thousands of identical anchors for
  a given page to be indexed but will affect the search scoring (i.e. tf=1.0f).
  </description>
</property>

<!-- indexingfilter plugin properties -->


<!-- URL normalizer properties -->

<property>
  <name>urlnormalizer.loop.count</name>
  <value>2</value><!-- 1 -->
  <description>Optionally loop through normalizers several times, to make
  sure that all transformations have been performed.
  </description>
</property>

<!-- mime properties -->

<!-- plugin properties -->

<property>
  <name>plugin.folders</name>
  <value>plugins</value>
  <description>Directories where nutch plugins are located.  Each
  element may be a relative or absolute path.  If absolute, it is used
  as is.  If relative, it is searched for on the classpath.</description>
</property>

<property>
  <name>plugin.auto-activation</name>
  <value>true</value>
  <description>Defines if some plugins that are not activated regarding
  the plugin.includes and plugin.excludes properties must be automaticaly
  activated if they are needed by some actived plugins.
  </description>
</property>

<property>
  <name>plugin.includes</name>
  <value>protocol-http|protocol-httpclient|urlfilter-regex|parse-(html|tika)|index-(basic|anchor)|indexer-solr|scoring-opic|urlnormalizer-(pass|regex|basic)</value>
  <description>Regular expression naming plugin directory names to
  include.  Any plugin not matching this expression is excluded.
  In any case you need at least include the nutch-extensionpoints plugin. By
  default Nutch includes crawling just HTML and plain text via HTTP,
  and basic indexing and search plugins. In order to use HTTPS please enable 
  protocol-httpclient, but be aware of possible intermittent problems with the 
  underlying commons-httpclient library. Set parsefilter-naivebayes for classification based focused crawler.
  </description>
</property>

<property>
  <name>plugin.excludes</name>
  <value></value>
  <description>Regular expression naming plugin directory names to exclude.  
  </description>
</property>

<property>
  <name>urlmeta.tags</name>
  <value>description</value><!-- -->
  <description>
    To be used in conjunction with features introduced in NUTCH-655, which allows
    for custom metatags to be injected alongside your crawl URLs. Specifying those
    custom tags here will allow for their propagation into a pages outlinks, as
    well as allow for them to be included as part of an index.
    Values should be comma-delimited. ("tag1,tag2,tag3") Do not pad the tags with
    white-space at their boundaries, if you are using anything earlier than Hadoop-0.21. 
  </description>
</property>

<!-- parser properties -->

<property>
  <name>parser.html.form.use_action</name>
  <value>false</value>
  <description>If true, HTML parser will collect URLs from form action
  attributes. This may lead to undesirable behavior (submitting empty
  forms during next fetch cycle). If false, form action attribute will
  be ignored.</description>
</property>

<property>
  <name>parser.html.outlinks.ignore_tags</name>
  <value>script,link</value>
  <description>Comma separated list of HTML tags, from which outlinks 
  shouldn't be extracted. Nutch takes links from: a, area, form, frame, 
  iframe, script, link, img. If you add any of those tags here, it
  won't be taken. Default is empty list. Probably reasonable value
  for most people would be "img,script,link".</description>
</property>

<property>
  <name>parsefilter.naivebayes.wordlist</name>
  <value>naivebayes-wordlist.txt</value>
  <description>Put the name of the file you want to be used as a list of 
  important words to be matched in the url for the model filter. The format should be one word per line.
  </description>
</property>

<property>
  <name>parser.timeout</name>
  <value>30</value>
  <description>Timeout in seconds for the parsing of a document, otherwise treats it as an exception and 
  moves on the the following documents. This parameter is applied to any Parser implementation. 
  Set to -1 to deactivate, bearing in mind that this could cause
  the parsing to crash because of a very long or corrupted document.
  </description>
</property>

<property>
  <name>parse.filter.urls</name>
  <value>true</value>
  <description>Whether the parser will filter URLs (with the configured URL filters).</description>
</property>

<property>
  <name>parse.normalize.urls</name>
  <value>true</value>
  <description>Whether the parser will normalize URLs (with the configured URL normalizers).</description>
</property>

<property>
  <name>parser.skip.truncated</name>
  <value>true</value>
  <description>Boolean value for whether we should skip parsing for truncated documents. By default this 
  property is activated due to extremely high levels of CPU which parsing can sometimes take.  
  </description>
</property>

<!-- urlfilter plugin properties -->

<!-- scoring filters properties -->

<!-- language-identifier plugin properties -->

<!-- index-static plugin properties -->


<!-- index-metadata plugin properties -->

<property>
  <name>index.parse.md</name>
  <value>*</value><!-- Added metatag.description,metatag.keywords -->
  <description>
  Comma-separated list of keys to be taken from the parse metadata to generate fields.
  Can be used e.g. for 'description' or 'keywords' provided that these values are generated
  by a parser (see parse-metatags plugin)  
  </description>
</property>

<property>
  <name>index.content.md</name>
  <value>*</value><!-- Instead of empty -->
  <description>
   Comma-separated list of keys to be taken from the content metadata to generate fields. 
  </description>
</property>

<property>
  <name>index.db.md</name>
  <value>*</value>
  <description>
     Comma-separated list of keys to be taken from the crawldb metadata to generate fields.
     Can be used to index values propagated from the seeds with the plugin urlmeta 
  </description>
</property>

<!-- index-geoip plugin properties -->
<property>
  <name>index.replace.regexp</name>
  <value/>
  <description>Allows indexing-time regexp replace manipulation of metadata fields.
    The format of the property is a list of regexp replacements, one line per field being
    modified.  Include index-replace in your plugin.includes.

    Example:
        hostmatch=.*somedomain.com
        fldname1=/regexp/replacement/flags
        fldname2=/regexp/replacement/flags

    Field names would be one of those from https://wiki.apache.org/nutch/IndexStructure.
    See https://wiki.apache.org/nutch/IndexReplace for further details.
  </description>
</property>

<!-- parse-metatags plugin properties -->
<property>
  <name>metatags.names</name>
  <value>*</value>
  <description> Names of the metatags to extract, separated by ','.
  Use '*' to extract all metatags. Prefixes the names with 'metatag.'
  in the parse-metadata. For instance to index description and keywords, 
  you need to activate the plugin index-metadata and set the value of the 
  parameter 'index.parse.md' to 'metatag.description,metatag.keywords'.
  </description>
</property>

<!-- Temporary Hadoop 0.17.x workaround. -->

<!-- linkrank scoring properties -->

<property>
  <name>link.ignore.internal.host</name>
  <value>false</value>
  <description>Ignore outlinks to the same hostname.</description>
</property>

<property>
  <name>link.ignore.internal.domain</name>
  <value>false</value>
  <description>Ignore outlinks to the same domain.</description>
</property>

<property>
  <name>link.ignore.limit.page</name>
  <value>false</value>
  <description>Limit to only a single outlink to the same page.</description>
</property>

<property>
  <name>link.ignore.limit.domain</name>
  <value>flase</value>
  <description>Limit to only a single outlink to the same domain.</description>
</property> 

<!-- solr index properties -->

<!-- Elasticsearch properties -->

<!-- subcollection properties -->

<!-- Headings plugin properties -->

<property>
  <name>headings</name>
  <value>h1,h2,h3,h4</value>
  <description>Comma separated list of headings to retrieve from the document</description>
</property>

<!-- mimetype-filter plugin properties -->

<!-- protocol-selenium plugin properties -->

<property>
  <name>selenium.driver</name>
  <value>remote</value>
  <description>
    A String value representing the flavour of Selenium 
    WebDriver() to use. Currently the following options
    exist - 'firefox', 'chrome', 'safari', 'opera' and 'remote'.
    If 'remote' is used it is essential to also set correct properties for
    'selenium.hub.port', 'selenium.hub.path', 'selenium.hub.host',
    'selenium.hub.protocol', 'selenium.grid.driver' and 'selenium.grid.binary'.
  </description>
</property>

<property>
  <name>selenium.take.screenshot</name>
  <value>false</value>
  <description>
    Boolean property determining whether the protocol-selenium
    WebDriver should capture a screenshot of the URL. If set to
    true remember to define the 'selenium.screenshot.location' 
    property as this determines the location screenshots should be 
    persisted to on HDFS. If that property is not set, screenshots
    are simply discarded.
  </description>
</property>

<property>
  <name>selenium.screenshot.location</name>
  <value></value>
  <description>
    The location on disk where a URL screenshot should be saved
    to if the 'selenium.take.screenshot' proerty is set to true.
    By default this is null, in this case screenshots held in memory
    are simply discarded.
  </description>
</property>

<property>
  <name>selenium.hub.port</name>
  <value>4444</value>
  <description>Selenium Hub Location connection port</description>
</property>

<property>
  <name>selenium.hub.path</name>
  <value>/wd/hub</value>
  <description>Selenium Hub Location connection path</description>
</property>

<property>
  <name>selenium.hub.host</name>
  <value>localhost</value>
  <description>Selenium Hub Location connection host</description>
</property>

<property>
  <name>selenium.hub.protocol</name>
  <value>http</value>
  <description>Selenium Hub Location connection protocol</description>
</property>

<property>
  <name>selenium.grid.driver</name>
  <value>firefox</value>
  <description>A String value representing the flavour of Selenium 
    WebDriver() used on the selenium grid. Currently the following options
    exist - 'firefox' </description>
</property>

<property>
  <name>selenium.grid.binary</name>
  <value></value>
  <description>A String value representing the path to the browser binary 
    location for each node
 </description>
</property>

<!-- lib-selenium configuration -->
<property>
  <name>libselenium.page.load.delay</name>
  <value>3</value>
  <description>
    The delay in seconds to use when loading a page with lib-selenium. This
    setting is used by protocol-selenium and protocol-interactiveselenium
    since they depending on lib-selenium for fetching.
  </description>
</property>

<!-- protocol-interactiveselenium configuration -->
<property>
  <name>interactiveselenium.handlers</name>
  <value>NavigationHandler</value>
  <description>
    A comma separated list of Selenium handlers that should be run for a given
    URL. The DefaultHandler causes the same functionality as protocol-selenium.
    Custom handlers can be implemented in the plugin package and included here.
  </description>
</property>

<property>
  <name>store.http.request</name>
  <value>false</value>
  <description>
    Store the raw request made by Nutch, required to use the CommonCrawlDataDumper
    tool for the WARC format.
  </description>
</property>

<property>
  <name>store.http.headers</name>
  <value>false</value>
  <description>
    Store the raw headers received by Nutch from the server, required to use the 
    CommonCrawlDataDumper tool for the WARC format.
  </description>
</property>

<!-- Hadoop log directory -->

<property>
  <name>parser.skip.truncated</name>
  <value>false</value>
  <description>Boolean value for whether we should skip parsing for truncated documents. By default this
  property is activated due to extremely high levels of CPU which parsing can sometimes take.
  </description>
</property>


</configuration>

